# FlexiMart Data Architecture Project

**Student Name:** Areti Sai Sreeja

**Student ID:** bitsom_ba_25071071

**Email:** asreejareddy27@gmail.com

**Date:** 08/01/2026

---

## 1. Project Overview

The **FlexiMart Data Architecture Project** demonstrates the design and implementation of an end-to-end data management solution for a retail business. The project covers relational database modeling, ETL pipeline development, NoSQL database evaluation, and data warehousing with analytical reporting.

The primary objective is to show how raw transactional data can be transformed into structured, scalable, and analytics-ready systems that support informed business decision-making.

---

## 2. Repository Structure

```
studentID-fleximart-data-architecture/
│
├── README.md
├── .gitignore
│
├── data/
│   ├── customers_raw.csv
│   ├── products_raw.csv
│   ├── sales_raw.csv
│   ├── customers_cleaned.csv
│   ├── products_cleaned.csv
│   ├── order_items_cleaned.csv
│
├── part1-database-etl/
│   ├── README.md
│   ├── etl_pipeline.py
│   ├── schema_documentation.md
│   ├── business_queries.sql
│   ├── data_quality_report.txt
│   └── requirements.txt
│
├── part2-nosql/
│   ├── README.md
│   ├── nosql_analysis.md
│   ├── mongodb_operations.js
│   └── products_catalog.json
│
└── part3-datawarehouse/
    ├── README.md
    ├── star_schema_design.md
    ├── warehouse_schema.sql
    ├── warehouse_data.sql
    └── analytics_queries.sql
```

---

## 3. Input Data Description (`data/` Folder)

The `data/` directory contains raw source files used as inputs to the ETL pipeline. These datasets are treated as immutable raw data and are not modified directly.

### customers_raw.csv

* Stores customer master data
* Key attributes:

  * customer_id
  * first_name, last_name
  * email
  * city, state
  * registration_date
* Used to populate the `customers` table

### products_raw.csv

* Contains product master information
* Key attributes:

  * product_id
  * product_name
  * category, subcategory
  * unit_price
  * stock_quantity
* Used to populate the `products` table

### sales_raw.csv

* Stores transaction-level sales data
* Key attributes:

  * order_id
  * order_date
  * customer_id
  * product_id
  * quantity
  * unit_price
* Used to populate `orders` and `order_items` tables

---

## 4. Cleaned Data Files Generated by ETL Pipeline

After successful execution of the ETL pipeline, four cleaned and standardized datasets are generated. These files are validated, structured, and analytics-ready.

---

### 4.1 customers_cleaned.csv

**Description:**
Contains cleaned customer master data loaded into the `customers` table.

**Cleaning & Transformation Applied:**

* Removed duplicate customer records
* Standardized column names
* Validated mandatory fields
* Ensured unique email addresses
* Normalized registration date format

**Key Columns:**

* first_name
* last_name
* email
* phone
* city
* registration_date

**Purpose:**
Acts as the master customer reference and parent table for orders.

---

### 4.2 products_cleaned.csv

**Description:**
Contains standardized product master data loaded into the `products` table.

**Cleaning & Transformation Applied:**

* Removed duplicate products
* Standardized category and subcategory values
* Validated numeric fields
* Filled missing stock values with defaults

**Key Columns:**

* product_name
* category
* subcategory
* price
* stock_quantity

**Purpose:**
Provides a consistent product reference for sales analysis.

---

### 4.3 orders_cleaned.csv

**Description:**
Represents aggregated order-level data derived from transactional sales.

**Transformation Logic Applied:**

* Grouped sales by customer and transaction date
* Calculated total order amount
* Assigned default order status as *Pending*
* Ensured valid order dates

**Key Columns:**

* customer_id
* transaction_date
* total_amount
* status

**Purpose:**
Stores one record per order and acts as a parent table for order items.

---

### 4.4 order_items_cleaned.csv

**Description:**
Contains detailed line-item sales data corresponding to each order.

**Cleaning & Transformation Applied:**

* Removed invalid rows
* Calculated subtotal (quantity × unit_price)
* Ensured numeric consistency

**Key Columns:**

* order_id
* product_id
* quantity
* unit_price
* subtotal

**Purpose:**
Supports detailed sales analysis with a many-to-one relationship to orders.

---

### Summary of Cleaned Data

The cleaned datasets ensure:

* Referential integrity across tables
* Elimination of duplicates and invalid records
* Standardized formats suitable for database loading
* Readiness for business queries and analytics

Together, these datasets form the foundation for reporting and data warehouse integration.

---

## 5. Part 1: Database Design & ETL Pipeline

This section focuses on relational database design, normalization, ETL processing, and SQL-based business reporting.

**Key Components:**

* `etl_pipeline.py`: End-to-end ETL processing and data validation
* `schema_documentation.md`: Entity relationships and normalization explanation
* `business_queries.sql`: Business-oriented SQL queries
* `data_quality_report.txt`: Auto-generated data validation summary

---

## 6. Part 2: NoSQL Database Analysis (MongoDB)

This section evaluates MongoDB for managing a flexible and diverse product catalog.

**Key Components:**

* `nosql_analysis.md`: RDBMS limitations, MongoDB benefits, and trade-offs
* `products_catalog.json`: Semi-structured product data
* `mongodb_operations.js`: MongoDB queries, aggregations, and updates

**MongoDB Concepts Demonstrated:**

* Flexible schema design
* Embedded documents
* Aggregation pipelines

---

## 7. Part 3: Data Warehouse & Analytics

This section implements a dimensional data warehouse using a star schema and performs OLAP analytics.

**Key Components:**

* `star_schema_design.md`: Schema design and modeling decisions
* `warehouse_schema.sql`: Dimension and fact table creation
* `warehouse_data.sql`: Realistic warehouse data population
* `analytics_queries.sql`: Drill-down, product, and customer analytics

**Supported Analytics:**

* Drill-down and roll-up analysis
* Sales trend analysis
* Customer and product performance insights

---

## 8. Technologies Used

* Python 3.x or Jupyter Notebook (pandas, mysql-connector-python)
* MySQL 8.0
* MongoDB 6.0
* SQL (OLTP & OLAP)


---

## 9. Setup Instructions

### Relational Database & ETL

```bash
python part1-database-etl/etl_pipeline.py
mysql -u root -p fleximart < part1-database-etl/business_queries.sql
```

### Data Warehouse

```bash
mysql -u root -p fleximart_dw < part3-datawarehouse/warehouse_schema.sql
mysql -u root -p fleximart_dw < part3-datawarehouse/warehouse_data.sql
mysql -u root -p fleximart_dw < part3-datawarehouse/analytics_queries.sql
```

### MongoDB

```bash
mongosh < part2-nosql/mongodb_operations.js
```

---

## 10. Key Learnings

* Designing normalized relational databases
* Building reliable ETL pipelines
* Evaluating NoSQL databases for flexibility
* Implementing star schema data warehouses
* Writing analytical SQL queries

---

## 11. Challenges Faced

1. **Managing foreign key dependencies during ETL**
   *Solution:* Sequential data loading and validation

2. **Generating realistic warehouse data patterns**
   *Solution:* Simulated weekend sales spikes and varied customer behavior

---

**End of Document**
